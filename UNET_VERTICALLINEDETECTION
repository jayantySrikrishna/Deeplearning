import numpy as np 
import os
import skimage.io as io
import skimage.transform as trans
import numpy as np
import cv2
import matplotlib.pyplot as plt
import pathlib
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
from tensorflow.keras import backend as keras
input_size = (640, 480,3)


import tensorflow as tf
# detect and init the TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)

# instantiate a distribution strategy
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
# instantiating the model in the strategy scope creates the model on the TPU

AUTO = tf.data.experimental.AUTOTUNE



batch_size = 16 * tpu_strategy.num_replicas_in_sync
print(batch_size)

data_dir = "../input/gridlines-dataset/NOISY_GRID/"

data_path = pathlib.Path(data_dir)

print('Database Directory')
print(data_path)

data_dir2 = "../input/gridlines-dataset/VER_GRID/"

data_path2 = pathlib.Path(data_dir2)
print('Database Directory')
print(data_path2)

image_count = len(list(data_path.glob('*.png')))
print(image_count)

X_dataset = sorted(list(data_path.glob('*')))
Y_dataset = sorted(list(data_path2.glob('*')))



#Preprocessing
training_data = []
output_data = []

def read_image(path, data):
    count = 0
    for img in path:
        count = count + 1
        im_path = str(img)
        img_array = cv2.imread(im_path)
        img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
        #img_array = np.memmap(im_path, dtype='float32', mode='r+', shape=(576, 544, 3))
        #img_array = cv2.resize(img_array, dsize=(576, 544), interpolation=cv2.INTER_AREA)
        data.append(img_array)
       
def read_gray_image(path, data):
    count = 0
    for img in path:
        count = count + 1
        im_path = str(img)
        img_array = cv2.imread(im_path, 0)
        #img_array = np.memmap(im_path, dtype='float32', mode='r+', shape=(576, 544, 3))
        #img_array = cv2.resize(img_array, dsize=(576, 544), interpolation=cv2.INTER_AREA)
        data.append(img_array)    

read_image(X_dataset, training_data) 
X_dataset = None
read_gray_image(Y_dataset, output_data) 
Y_dataset = None
print(len(training_data))
print(len(output_data))

#input
X = np.array(training_data)
X = X.astype('float32') / 255.0
training_data = None
#output

Y = np.array(output_data)
Y = Y.astype('float32') / 255.0

output_data = None

IMG_SHAPE = X.shape[1:]
OUTPUT_SHAPE = Y.shape[1:]
print(IMG_SHAPE)
print(OUTPUT_SHAPE)
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.2)

print("Start")
print(len(X_train))
print(len(X_test))
print("End")

# plot image example from training images
plt.imshow(X_train[1])
plt.show()

plt.imshow(X_test[1])
plt.show()

with tpu_strategy.scope():
      
    inputs = Input(IMG_SHAPE)
    conv1 = Conv2D(64, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(inputs)
    conv1 = Conv2D(64, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(pool1)
    conv2 = Conv2D(128, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(pool2)
    conv3 = Conv2D(256, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(pool3)
    conv4 = Conv2D(512, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv4)
    drop4 = Dropout(0.5)(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

    conv5 = Conv2D(1024, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(pool4)
    conv5 = Conv2D(1024, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv5)
    drop5 = Dropout(0.5)(conv5)

    up6 = Conv2D(512, (2,2), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(Conv2DTranspose(filters=512, kernel_size=3, padding='same', strides=2)(drop5))
    merge6 = concatenate([drop4,up6], axis = 3)
    conv6 = Conv2D(512, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(merge6)
    conv6 = Conv2D(512, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv6)

    up7 = Conv2D(256, (2,2), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(Conv2DTranspose(filters=256, kernel_size=3, padding='same', strides=2)(conv6))
    merge7 = concatenate([conv3,up7], axis = 3)
    conv7 = Conv2D(256, (3,3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(merge7)
    conv7 = Conv2D(256, (3,3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv7)

    up8 = Conv2D(128, (2,2), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(Conv2DTranspose(filters=128, kernel_size=3, padding='same', strides=2)(conv7))
    merge8 = concatenate([conv2,up8], axis = 3)
    conv8 = Conv2D(128, (3,3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(merge8)
    conv8 = Conv2D(128, (3,3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv8)

    up9 = Conv2D(64, (2,2), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(Conv2DTranspose(filters=64, kernel_size=3, padding='same', strides=2)(conv8))
    merge9 = concatenate([conv1,up9], axis = 3)
    conv9 = Conv2D(64, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(merge9)
    conv9 = Conv2D(64, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv9)
    conv9 = Conv2D(2, (3, 3), activation = tf.keras.layers.LeakyReLU(alpha=0.2), padding = 'same', use_bias=True, kernel_initializer = 'he_normal')(conv9)
    conv10 = Conv2D(1, (1,1), activation = 'sigmoid')(conv9)

    model = Model(inputs = (inputs), outputs = conv10)

    model.compile(optimizer ='adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])
    
   # model.summary()

    print(model.summary())

    history = model.fit(x=X_train, y=Y_train, epochs=100,
                validation_data=[X_test, Y_test])

    

import numpy as np
predictions = np.clip(model.predict(X_test), 0.0, 1.0)
plt.imshow(X_test[10])
plt.show()
#plt.imshow(predictions[10])
#plt.show()
for i in range(20, 35):
    arr = np.uint8(X_test[i]*255)
    test = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)
    pred = predictions[i]
    cv2.imwrite("noisy_"+str(i)+".png", test)
    cv2.imwrite("denoised_"+str(i)+".png", 255*pred)

import h5py
model.save_weights('model_grid.h5')
print('Done!')    
